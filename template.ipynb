{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Severity of Major Power Outages in the U.S.\n",
    "\n",
    "**Name(s)**: Pratham Aggarwal\n",
    "\n",
    "**Website Link**: https://pratham-aggr.github.io/power_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dsc80_utils import *\n",
    "import folium\n",
    "import requests\n",
    "from scipy import stats\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\"\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nb Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_lists():\n",
    "    categorical_vars = [\n",
    "        'u.s._state',\n",
    "        'nerc.region',\n",
    "        'climate.region',\n",
    "        'climate.category',\n",
    "        'cause.category',\n",
    "        'cause.category.detail'\n",
    "    ]\n",
    "\n",
    "    numeric_vars = [\n",
    "        'anomaly.level (numeric)',\n",
    "        'demand.loss.mw (megawatt)',\n",
    "        'customers.affected',\n",
    "        'res.price (cents / kilowatt-hour)',\n",
    "        'com.price (cents / kilowatt-hour)',\n",
    "        'ind.price (cents / kilowatt-hour)',\n",
    "        'total.price (cents / kilowatt-hour)',\n",
    "        'res.sales (megawatt-hour)',\n",
    "        'com.sales (megawatt-hour)',\n",
    "        'ind.sales (megawatt-hour)',\n",
    "        'total.sales (megawatt-hour)',\n",
    "        'res.percen (%)',\n",
    "        'com.percen (%)',\n",
    "        'ind.percen (%)',\n",
    "        'res.customers',\n",
    "        'com.customers',\n",
    "        'ind.customers',\n",
    "        'total.customers',\n",
    "        'res.cust.pct (%)',\n",
    "        'com.cust.pct (%)',\n",
    "        'ind.cust.pct (%)',\n",
    "        'pc.realgsp.state (usd)',\n",
    "        'pc.realgsp.usa (usd)',\n",
    "        'pc.realgsp.rel (fraction)',\n",
    "        'pc.realgsp.change (%)',\n",
    "        'util.realgsp (usd)',\n",
    "        'total.realgsp (usd)',\n",
    "        'util.contri (%)',\n",
    "        'pi.util.ofusa (%)',\n",
    "        'population',\n",
    "        'poppct_urban (%)',\n",
    "        'poppct_uc (%)',\n",
    "        'popden_urban (persons per square mile)',\n",
    "        'popden_uc (persons per square mile)',\n",
    "        'popden_rural (persons per square mile)',\n",
    "        'areapct_urban (%)',\n",
    "        'areapct_uc (%)',\n",
    "        'pct_land (%)',\n",
    "        'pct_water_tot (%)',\n",
    "        'pct_water_inland (%)'\n",
    "    ]\n",
    "\n",
    "    datetime_vars = [\n",
    "        'outage_start',\n",
    "        'outage_restore'\n",
    "    ]\n",
    "\n",
    "    return categorical_vars, numeric_vars, datetime_vars\n",
    "\n",
    "def plot_single_bar(df,col,color = 'blue'):\n",
    "    vc = df[col].value_counts(normalize=True).reset_index()\n",
    "    vc.columns = [col, 'proportion']\n",
    "    fig = make_subplots(rows = 1, cols = 1, subplot_titles = [col])\n",
    "    fig.add_trace(go.Bar(x=vc[col], y=vc['proportion'], marker_color = color), row=1, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgb(243, 243, 243)',\n",
    "        plot_bgcolor='rgb(243, 243, 243)'\n",
    "    )\n",
    "    fig.write_html(f'assets/univariate_analysis_{col}.html')\n",
    "    return fig\n",
    "    #plotly subplots reference: https://plotly.com/python/subplots\n",
    "\n",
    "def plot_multiple_bars(df, columns ,title = 'Distributions'):\n",
    "    n = len(columns)\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=columns)\n",
    "    row, col = 1, 1\n",
    "    \n",
    "    for var in columns:\n",
    "        vc = df[var].value_counts(normalize=True).reset_index()\n",
    "        vc.columns = [var, 'proportion']\n",
    "        fig.add_trace(go.Bar(x=vc[var], y=vc['proportion']), row=row, col=col)\n",
    "        col += 1\n",
    "        if col > cols:\n",
    "            col = 1\n",
    "            row += 1\n",
    "            \n",
    "    fig.update_layout(\n",
    "        height=500 * rows, \n",
    "        width=1200, \n",
    "        title=title,\n",
    "        paper_bgcolor='rgb(243, 243, 243)',\n",
    "        plot_bgcolor='rgb(243, 243, 243)'\n",
    "    )\n",
    "    return fig\n",
    "    \n",
    "def plot_state_choropleth(df, value_col, aggfunc = 'mean', title =''):\n",
    "    map_df = df.groupby('u.s._state')[value_col].agg(aggfunc).reset_index()    \n",
    "    geojson_url = \"https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json\"\n",
    "    us_states = requests.get(geojson_url).json()\n",
    "    \n",
    "    fig = px.choropleth(\n",
    "        map_df,\n",
    "        geojson = us_states,\n",
    "        locations = 'u.s._state',\n",
    "        featureidkey=\"properties.name\", \n",
    "        color = value_col,\n",
    "        color_continuous_scale = 'YlGn',\n",
    "        scope = 'usa',\n",
    "        labels = 'value_col.title()',\n",
    "        title=title\n",
    "    )\n",
    "    fig.update_geos(\n",
    "        bgcolor=\"rgb(243,243,243)\",\n",
    "        visible=False\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=\"rgb(243,243,243)\",  # outer background\n",
    "        plot_bgcolor=\"rgb(243,243,243)\",   # around the map\n",
    "        title_x=0.5\n",
    "    )\n",
    "    fig.write_html('assets/map.html')\n",
    "    return fig\n",
    "    #px choropleth reference: https://plotly.com/python/choropleth-maps\n",
    "#moduified from dsc utils\n",
    "def create_kde_plotly(df, group_col, group1, group2, vals_col, title=''):\n",
    "    fig = ff.create_distplot(\n",
    "        hist_data=[df.loc[df[group_col] == group1, vals_col], df.loc[df[group_col] == group2, vals_col]],\n",
    "        group_labels=[group1, group2],\n",
    "        show_rug=False, show_hist=False\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        paper_bgcolor='rgb(243, 243, 243)',\n",
    "        plot_bgcolor='rgb(243, 243, 243)',\n",
    "        width=600,\n",
    "        height=500\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dct = https://www.sciencedirect.com/science/article/pii/S2352340918307182\n",
    "fp = Path('data') / 'outage.csv'\n",
    "raw_df = pd.read_csv(fp, skiprows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.shape\n",
    "#summary stats, col description are in section Step 2: Data Cleaning and Exploratory Data Analysis \n",
    "#because the data requires a little bit of cleaning before displaying anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "df = raw_df.copy(deep=True)\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "units = df.iloc[0]\n",
    "\n",
    "df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "new_columns = []\n",
    "for col, unit in zip(df.columns, units):\n",
    "    if pd.notna(unit):\n",
    "        new_columns.append(f\"{col} ({unit})\")\n",
    "    else:\n",
    "        new_columns.append(col)\n",
    "\n",
    "df.columns = new_columns\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "categorical_vars, numeric_vars, datetime_vars = get_variable_lists()\n",
    "\n",
    "for col in numeric_vars:\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "#state abbreiation are df['postal.code'] hence drop it\n",
    "#dropping outage start and restore and making a single col for start time/date and end; month/year is redundant info\n",
    "#hurrican name is useless for our analysis\n",
    "df = df.rename(columns={\n",
    "    \"outage.start.date (day of the week, month day, year)\": \"start_date\",\n",
    "    \"outage.start.time (hour:minute:second (am / pm))\": \"start_time\",\n",
    "    \"outage.restoration.date (day of the week, month day, year)\": \"restore_date\",\n",
    "    \"outage.restoration.time (hour:minute:second (am / pm))\": \"restore_time\"\n",
    "})\n",
    "fmt = \"%A, %B %d, %Y %I:%M:%S %p\"\n",
    "\n",
    "df[\"outage_start\"] = pd.to_datetime(df[\"start_date\"] + \" \" + df[\"start_time\"], format=fmt)\n",
    "df[\"outage_restore\"] = pd.to_datetime(df[\"restore_date\"] + \" \" + df[\"restore_time\"], format=fmt)\n",
    "df[\"dur_hours\"] = (df[\"outage_restore\"] - df[\"outage_start\"]).dt.total_seconds() / 3600\n",
    "\n",
    "#since outage.duration is linearly dependent on outage_start & outage_restore, to I will drop it to avoid redundant info\n",
    "#cause.category.detail is missing by design so even filling the values won't influence things a lot so would drop it\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"start_date\", \n",
    "        \"start_time\", \n",
    "        \"restore_date\", \n",
    "        \"restore_time\", \n",
    "        \"year\", \n",
    "        \"month\", \n",
    "        \"hurricane.names\",\n",
    "        \"outage.duration (mins)\",\n",
    "        'obs',\n",
    "        'variables (units)', \n",
    "        'postal.code',\n",
    "        'outage_start',\n",
    "        'outage_restore',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "important_cols = [\n",
    "    'dur_hours',\n",
    "    'customers.affected',\n",
    "    'demand.loss.mw (megawatt)',\n",
    "    'population',\n",
    "    'poppct_urban (%)',\n",
    "    'res.price (cents / kilowatt-hour)',\n",
    "    'pc.realgsp.state (usd)'\n",
    "]\n",
    "df.describe()[important_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_multiple_bars(df, categorical_vars ,title = 'Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_bar(df,'cause.category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_bar(df,'climate.region', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(\n",
    "    df,\n",
    "    x = 'cause.category',\n",
    "    y = 'dur_hours',\n",
    "    title = 'Average Outage Duration by Casue Category'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height = 500,\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)'\n",
    ")\n",
    "fig.update_yaxes(type='log') #for visibility since vanilla plot is not legible\n",
    "fig.write_html('assets/bivariate_analysis_cause.category_vs_dur_hours_box_plot.html')\n",
    "fig.show()\n",
    "#courtesy plotly boxplots reference: https://plotly.com/python/box-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_choropleth(df, 'dur_hours', \n",
    "                      aggfunc = 'mean',\n",
    "                      title='Average Duration of Major Power Outages (Hours) by U.S. State'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code groups states into 5 urbanization categories using quantile bins \n",
    "#and compute stats for outage durations for each group\n",
    "df['urban_bin'] = pd.qcut(\n",
    "    df['poppct_urban (%)'],\n",
    "    5,\n",
    "    labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "pivot = (\n",
    "    df.groupby('urban_bin')['dur_hours']\n",
    "    .agg(['count', 'mean', 'median'])\n",
    "    .round(2)\n",
    ")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['urban_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = create_kde_plotly(\n",
    "    df = df, \n",
    "    group_col = 'anomaly.level (numeric)', \n",
    "    group1 = True, \n",
    "    group2 = False, \n",
    "    vals_col = col, \n",
    "    title=f'KDE: anomaly.level vs pct_water_tot (%)'\n",
    "    )\n",
    "fig1.write_html(f'assets/hyp_pct_water_tot (%).html')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = create_kde_plotly(\n",
    "    df = df, \n",
    "    group_col = 'anomaly.level (numeric)', \n",
    "    group1 = True, \n",
    "    group2 = False, \n",
    "    vals_col = col, \n",
    "    title=f'KDE: anomaly.level vs com.sales (megawatt-hour)'\n",
    "    )\n",
    "fig2.write_html(f'assets/hyp_com.sales (megawatt-hour).html')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_test(df, col1, col2):\n",
    "    missing = df[df[col1].isna()][col2].values\n",
    "    not_missing = df[df[col1].notna()][col2].values\n",
    "    obs = stats.ks_2samp(missing, not_missing).statistic\n",
    "    comb = np.concatenate([missing, not_missing])\n",
    "    perm_stats = []\n",
    "    for _ in range(10_000):\n",
    "        perm = np.random.permutation(comb)\n",
    "        perm_miss = perm[:len(missing)]\n",
    "        perm_not_miss = perm[len(missing):]\n",
    "        perm_stat = stats.ks_2samp(perm_miss, perm_not_miss).statistic\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    p_val = np.mean(perm_stats >= obs)\n",
    "    return p_val        \n",
    "\n",
    "pval_no = perm_test(df, 'anomaly.level (numeric)', 'pct_water_tot (%)')\n",
    "pval_yes = perm_test(df, 'anomaly.level (numeric)', 'com.sales (megawatt-hour)')\n",
    "\n",
    "print(pval_no, pval_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perm test to examine whether outage duration depends on climate.category (there is an obvious yes atm)\n",
    "df_valid = df.dropna(subset=['climate.category', 'dur_hours']).copy()\n",
    "df_valid['is_normal_climate'] = (df_valid['climate.category']=='normal')\n",
    "\n",
    "fig = create_kde_plotly(\n",
    "    df = df_valid, \n",
    "    group_col = 'is_normal_climate', \n",
    "    group1 = True, \n",
    "    group2 = False, \n",
    "    vals_col = 'dur_hours', \n",
    "    title=f'KDE: climate.category vs dur_hours'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "#since dist shape is quite similar, I will be using ks stat\n",
    "def ks_perm_test_gen(df, col1, group, col2):\n",
    "    g1 = df[df[col1] == group][col2].dropna().values\n",
    "    g2 = df[df[col1]!=group][col2].dropna().values\n",
    "    obs = stats.ks_2samp(g1, g2).statistic\n",
    "    comb = np.concatenate([g1, g2])\n",
    "    perm_stats = []\n",
    "    for _ in range(10_000):\n",
    "        perm =np.random.permutation(comb)\n",
    "        perm_g1 = perm[:len(g1)]\n",
    "        perm_g2 = perm[len(g1):]\n",
    "        perm_stat = stats.ks_2samp(perm_g1, perm_g2).statistic\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    p_val = np.mean(perm_stats >= obs)\n",
    "    return p_val\n",
    "\n",
    "pval = ks_perm_test_gen(df_valid, 'is_normal_climate', True, 'dur_hours')\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "#predicting dur.hours based on almost all the feature present in the df\n",
    "df = df.drop(columns = ['cause.category.detail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "def model_pipeline(data, model):\n",
    "    categorical_features = [\n",
    "    'u.s._state',\n",
    "    'nerc.region',\n",
    "    'climate.region',\n",
    "    'climate.category',\n",
    "    'cause.category',\n",
    "    ]\n",
    "\n",
    "    num_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_features = [col for col in num_cols if col!='dur_hours']\n",
    "    \n",
    "    cat_proc = Pipeline(steps = [\n",
    "        ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    num_proc = Pipeline(steps = [\n",
    "        ('imputer', SimpleImputer(strategy = 'median')),\n",
    "        ('scale', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_proc, numeric_features),\n",
    "            ('cat', cat_proc, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\",model)\n",
    "    ])\n",
    "    return pipe\n",
    "    \n",
    "def train_model(data, model, gd=False, prm_grid=None):\n",
    "    data = data.dropna(subset=['dur_hours'])\n",
    "    X = data.drop(columns = ['dur_hours'])\n",
    "    y = data['dur_hours']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "    )        \n",
    "    if gd:\n",
    "        grid = GridSearchCV(\n",
    "            estimator = model,\n",
    "            param_grid = prm_grid,\n",
    "            scoring = 'neg_mean_absolute_error',\n",
    "            cv = 3,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        grid.fit(X_train, y_train)\n",
    "        print(\"Best Params:\", grid.best_params_)\n",
    "        model = grid.best_estimator_\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    prd_test, prd_train = model.predict(X_test), model.predict(X_train)\n",
    "    mae_train = np.mean(np.abs(prd_train-y_train))\n",
    "    mae_test = np.mean(np.abs(prd_test-y_test))\n",
    "    return model, mae_train, mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "base_model = model_pipeline(df_exp, LinearRegression())\n",
    "train_model(df_exp, base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only = True)\n",
    "fig = px.imshow(\n",
    "    corr,\n",
    "    text_auto = True,\n",
    "    color_continuous_scale = 'RdBu_r',\n",
    "    title = 'Correlation Matrix for Numerical features (Before Dropping)'\n",
    ")\n",
    "fig.write_html('assets/corr_before.html')\n",
    "fig.update_layout(width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df.copy(deep=True)\n",
    "df_exp = df_exp.drop(columns=[\n",
    "    'res.price (cents / kilowatt-hour)',\n",
    "    'com.price (cents / kilowatt-hour)',\n",
    "    'ind.price (cents / kilowatt-hour)',\n",
    "    'res.sales (megawatt-hour)',\n",
    "    'com.sales (megawatt-hour)', \n",
    "    'ind.sales (megawatt-hour)',\n",
    "    'total.sales (megawatt-hour)',\n",
    "    'res.customers', 'com.customers',\n",
    "    'ind.customers',\n",
    "    'com.cust.pct (%)', \n",
    "    'ind.cust.pct (%)',\n",
    "    'res.sales (megawatt-hour)',\n",
    "    'com.sales (megawatt-hour)', \n",
    "    'ind.sales (megawatt-hour)',\n",
    "    'com.percen (%)',\n",
    "    'ind.percen (%)',\n",
    "    'total.customers',\n",
    "    'util.realgsp (usd)',\n",
    "    'pi.util.ofusa (%)',\n",
    "    'pc.realgsp.rel (fraction)',\n",
    "    'pct_land (%)'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://plotly.com/python/heatmaps/\n",
    "#plotting the corr matrix to simpify the model\n",
    "\n",
    "corr = df_exp.corr(numeric_only = True)\n",
    "fig = px.imshow(\n",
    "    corr,\n",
    "    text_auto = True,\n",
    "    color_continuous_scale = 'RdBu_r',\n",
    "    title = 'Correlation Matrix for Numerical features (After Dropping)'\n",
    ")\n",
    "\n",
    "fig.write_html('assets/corr_after.html')\n",
    "\n",
    "fig.update_layout(width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [200, 400, 600],\n",
    "    \"model__max_depth\": [20, 40, None],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "rf = model_pipeline(df_exp, RandomForestRegressor())\n",
    "final_model, train_err, test_err = train_model(\n",
    "    df_exp, \n",
    "    rf, \n",
    "    gd = True, \n",
    "    prm_grid = param_grid\n",
    ")\n",
    "\n",
    "print(train_err, test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Best Params: {'model__criterion': 'absolute_error', 'model__max_depth': 40, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 5, 'model__n_estimators': 200}\n",
    "# 26.451434357344635 36.514895270270266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Params: {'model__max_depth': 1000, 'model__n_estimators': 400}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
