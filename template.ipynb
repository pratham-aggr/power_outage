{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Severity of Major Power Outages in the U.S.\n",
    "\n",
    "**Name(s)**: Pratham Aggarwal\n",
    "\n",
    "**Website Link**: https://pratham-aggr.github.io/power_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dsc80_utils import *\n",
    "import folium\n",
    "import requests\n",
    "from scipy import stats\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\"\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nb Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_lists():\n",
    "    categorical_vars = [\n",
    "        'u.s._state',\n",
    "        'nerc.region',\n",
    "        'climate.region',\n",
    "        'climate.category',\n",
    "        'cause.category',\n",
    "        'cause.category.detail'\n",
    "    ]\n",
    "\n",
    "    numeric_vars = [\n",
    "        'anomaly.level (numeric)',\n",
    "        'demand.loss.mw (megawatt)',\n",
    "        'customers.affected',\n",
    "        'res.price (cents / kilowatt-hour)',\n",
    "        'com.price (cents / kilowatt-hour)',\n",
    "        'ind.price (cents / kilowatt-hour)',\n",
    "        'total.price (cents / kilowatt-hour)',\n",
    "        'res.sales (megawatt-hour)',\n",
    "        'com.sales (megawatt-hour)',\n",
    "        'ind.sales (megawatt-hour)',\n",
    "        'total.sales (megawatt-hour)',\n",
    "        'res.percen (%)',\n",
    "        'com.percen (%)',\n",
    "        'ind.percen (%)',\n",
    "        'res.customers',\n",
    "        'com.customers',\n",
    "        'ind.customers',\n",
    "        'total.customers',\n",
    "        'res.cust.pct (%)',\n",
    "        'com.cust.pct (%)',\n",
    "        'ind.cust.pct (%)',\n",
    "        'pc.realgsp.state (usd)',\n",
    "        'pc.realgsp.usa (usd)',\n",
    "        'pc.realgsp.rel (fraction)',\n",
    "        'pc.realgsp.change (%)',\n",
    "        'util.realgsp (usd)',\n",
    "        'total.realgsp (usd)',\n",
    "        'util.contri (%)',\n",
    "        'pi.util.ofusa (%)',\n",
    "        'population',\n",
    "        'poppct_urban (%)',\n",
    "        'poppct_uc (%)',\n",
    "        'popden_urban (persons per square mile)',\n",
    "        'popden_uc (persons per square mile)',\n",
    "        'popden_rural (persons per square mile)',\n",
    "        'areapct_urban (%)',\n",
    "        'areapct_uc (%)',\n",
    "        'pct_land (%)',\n",
    "        'pct_water_tot (%)',\n",
    "        'pct_water_inland (%)'\n",
    "    ]\n",
    "\n",
    "    datetime_vars = [\n",
    "        'outage_start',\n",
    "        'outage_restore'\n",
    "    ]\n",
    "\n",
    "    return categorical_vars, numeric_vars, datetime_vars\n",
    "\n",
    "def plot_single_bar(df,col,color = 'blue'):\n",
    "    vc = df[col].value_counts(normalize=True).reset_index()\n",
    "    vc.columns = [col, 'proportion']\n",
    "    fig = make_subplots(rows = 1, cols = 1, subplot_titles = [col])\n",
    "    fig.add_trace(go.Bar(x=vc[col], y=vc['proportion'], marker_color = color), row=1, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgb(243, 243, 243)',\n",
    "        plot_bgcolor='rgb(243, 243, 243)'\n",
    "    )\n",
    "    fig.write_html(f'assets/univariate_analysis_{col}.html')\n",
    "    return fig\n",
    "    #plotly subplots reference: https://plotly.com/python/subplots\n",
    "\n",
    "def plot_multiple_bars(df, columns ,title = 'Distributions'):\n",
    "    n = len(columns)\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=columns)\n",
    "    row, col = 1, 1\n",
    "    \n",
    "    for var in columns:\n",
    "        vc = df[var].value_counts(normalize=True).reset_index()\n",
    "        vc.columns = [var, 'proportion']\n",
    "        fig.add_trace(go.Bar(x=vc[var], y=vc['proportion']), row=row, col=col)\n",
    "        col += 1\n",
    "        if col > cols:\n",
    "            col = 1\n",
    "            row += 1\n",
    "            \n",
    "    fig.update_layout(\n",
    "        height=500 * rows, \n",
    "        width=1200, \n",
    "        title=title,\n",
    "        paper_bgcolor='rgb(243, 243, 243)',\n",
    "        plot_bgcolor='rgb(243, 243, 243)'\n",
    "    )\n",
    "    return fig\n",
    "    \n",
    "def plot_state_choropleth(df, value_col, aggfunc = 'mean', title =''):\n",
    "    map_df = df.groupby('u.s._state')[value_col].agg(aggfunc).reset_index()    \n",
    "    geojson_url = \"https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json\"\n",
    "    us_states = requests.get(geojson_url).json()\n",
    "    \n",
    "    fig = px.choropleth(\n",
    "        map_df,\n",
    "        geojson = us_states,\n",
    "        locations = 'u.s._state',\n",
    "        featureidkey=\"properties.name\", \n",
    "        color = value_col,\n",
    "        color_continuous_scale = 'YlGn',\n",
    "        scope = 'usa',\n",
    "        labels = 'value_col.title()',\n",
    "        title=title\n",
    "    )\n",
    "    fig.update_geos(\n",
    "        fitbounds=\"locations\", \n",
    "        bgcolor=\"rgb(243,243,243)\",\n",
    "        visible=False\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=\"rgb(243,243,243)\",  # outer background\n",
    "        plot_bgcolor=\"rgb(243,243,243)\",   # around the map\n",
    "        title_x=0.5\n",
    "    )\n",
    "    fig.write_html('assets/map.html')\n",
    "    return fig\n",
    "    #px choropleth reference: https://plotly.com/python/choropleth-maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dct = https://www.sciencedirect.com/science/article/pii/S2352340918307182\n",
    "fp = Path('data') / 'outage.csv'\n",
    "raw_df = pd.read_csv(fp, skiprows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1535, 57)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape\n",
    "#summary stats, col description are in section Step 2: Data Cleaning and Exploratory Data Analysis \n",
    "#because the data requires a little bit of cleaning before displaying anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "df = raw_df.copy(deep=True)\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "units = df.iloc[0]\n",
    "\n",
    "df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "new_columns = []\n",
    "for col, unit in zip(df.columns, units):\n",
    "    if pd.notna(unit):\n",
    "        new_columns.append(f\"{col} ({unit})\")\n",
    "    else:\n",
    "        new_columns.append(col)\n",
    "\n",
    "df.columns = new_columns\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "categorical_vars, numeric_vars, datetime_vars = get_variable_lists()\n",
    "\n",
    "for col in numeric_vars:\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "#state abbreiation are df['postal.code'] hence drop it\n",
    "#dropping outage start and restore and making a single col for start time/date and end; month/year is redundant info\n",
    "#hurrican name is useless for our analysis\n",
    "df = df.rename(columns={\n",
    "    \"outage.start.date (day of the week, month day, year)\": \"start_date\",\n",
    "    \"outage.start.time (hour:minute:second (am / pm))\": \"start_time\",\n",
    "    \"outage.restoration.date (day of the week, month day, year)\": \"restore_date\",\n",
    "    \"outage.restoration.time (hour:minute:second (am / pm))\": \"restore_time\"\n",
    "})\n",
    "fmt = \"%A, %B %d, %Y %I:%M:%S %p\"\n",
    "\n",
    "df[\"outage_start\"] = pd.to_datetime(df[\"start_date\"] + \" \" + df[\"start_time\"], format=fmt)\n",
    "df[\"outage_restore\"] = pd.to_datetime(df[\"restore_date\"] + \" \" + df[\"restore_time\"], format=fmt)\n",
    "df[\"dur_hours\"] = (df[\"outage_restore\"] - df[\"outage_start\"]).dt.total_seconds() / 3600\n",
    "\n",
    "#since outage.duration is linearly dependent on outage_start & outage_restore, to I will drop it to avoid redundant info\n",
    "#cause.category.detail is missing by design so even filling the values won't influence things a lot so would drop it\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"start_date\", \n",
    "        \"start_time\", \n",
    "        \"restore_date\", \n",
    "        \"restore_time\", \n",
    "        \"year\", \n",
    "        \"month\", \n",
    "        \"hurricane.names\",\n",
    "        \"outage.duration (mins)\",\n",
    "        'obs',\n",
    "        'variables (units)', \n",
    "        'postal.code',\n",
    "        'outage_start',\n",
    "        'outage_restore',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur_hours</th>\n",
       "      <th>customers.affected</th>\n",
       "      <th>demand.loss.mw (megawatt)</th>\n",
       "      <th>population</th>\n",
       "      <th>poppct_urban (%)</th>\n",
       "      <th>res.price (cents / kilowatt-hour)</th>\n",
       "      <th>pc.realgsp.state (usd)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1476.00</td>\n",
       "      <td>1.09e+03</td>\n",
       "      <td>829.00</td>\n",
       "      <td>1.53e+03</td>\n",
       "      <td>1534.00</td>\n",
       "      <td>1512.00</td>\n",
       "      <td>1534.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.75</td>\n",
       "      <td>1.43e+05</td>\n",
       "      <td>536.29</td>\n",
       "      <td>1.32e+07</td>\n",
       "      <td>80.97</td>\n",
       "      <td>11.97</td>\n",
       "      <td>49390.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>99.07</td>\n",
       "      <td>2.87e+05</td>\n",
       "      <td>2196.45</td>\n",
       "      <td>1.16e+07</td>\n",
       "      <td>11.90</td>\n",
       "      <td>3.09</td>\n",
       "      <td>11687.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.68</td>\n",
       "      <td>7.01e+04</td>\n",
       "      <td>168.00</td>\n",
       "      <td>8.77e+06</td>\n",
       "      <td>84.05</td>\n",
       "      <td>11.46</td>\n",
       "      <td>48370.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.00</td>\n",
       "      <td>1.50e+05</td>\n",
       "      <td>400.00</td>\n",
       "      <td>1.94e+07</td>\n",
       "      <td>89.81</td>\n",
       "      <td>13.90</td>\n",
       "      <td>53622.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1811.88</td>\n",
       "      <td>3.24e+06</td>\n",
       "      <td>41788.00</td>\n",
       "      <td>3.93e+07</td>\n",
       "      <td>100.00</td>\n",
       "      <td>34.58</td>\n",
       "      <td>168377.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dur_hours  customers.affected  demand.loss.mw (megawatt)  population  \\\n",
       "count    1476.00            1.09e+03                     829.00    1.53e+03   \n",
       "mean       43.75            1.43e+05                     536.29    1.32e+07   \n",
       "std        99.07            2.87e+05                    2196.45    1.16e+07   \n",
       "...          ...                 ...                        ...         ...   \n",
       "50%        11.68            7.01e+04                     168.00    8.77e+06   \n",
       "75%        48.00            1.50e+05                     400.00    1.94e+07   \n",
       "max      1811.88            3.24e+06                   41788.00    3.93e+07   \n",
       "\n",
       "       poppct_urban (%)  res.price (cents / kilowatt-hour)  \\\n",
       "count           1534.00                            1512.00   \n",
       "mean              80.97                              11.97   \n",
       "std               11.90                               3.09   \n",
       "...                 ...                                ...   \n",
       "50%               84.05                              11.46   \n",
       "75%               89.81                              13.90   \n",
       "max              100.00                              34.58   \n",
       "\n",
       "       pc.realgsp.state (usd)  \n",
       "count                 1534.00  \n",
       "mean                 49390.12  \n",
       "std                  11687.43  \n",
       "...                       ...  \n",
       "50%                  48370.00  \n",
       "75%                  53622.00  \n",
       "max                 168377.00  \n",
       "\n",
       "[8 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_cols = [\n",
    "    'dur_hours',\n",
    "    'customers.affected',\n",
    "    'demand.loss.mw (megawatt)',\n",
    "    'population',\n",
    "    'poppct_urban (%)',\n",
    "    'res.price (cents / kilowatt-hour)',\n",
    "    'pc.realgsp.state (usd)'\n",
    "]\n",
    "df.describe()[important_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"1020\"\n",
       "    src=\"iframe_figures/figure_68.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multiple_bars(df, categorical_vars ,title = 'Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_69.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_single_bar(df,'cause.category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_70.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_single_bar(df,'climate.region', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_71.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.box(\n",
    "    df,\n",
    "    x = 'cause.category',\n",
    "    y = 'dur_hours',\n",
    "    title = 'Average Outage Duration by Casue Category'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height = 500,\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)'\n",
    ")\n",
    "fig.update_yaxes(type='log') #for visibility since vanilla plot is not legible\n",
    "fig.write_html('assets/bivariate_analysis_cause.category_vs_dur_hours_box_plot.html')\n",
    "fig.show()\n",
    "#courtesy plotly boxplots reference: https://plotly.com/python/box-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Minnesota', 'Tennessee', 'Wisconsin', ..., 'North Dakota',\n",
       "       'South Dakota', 'Alaska'], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['u.s._state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_73.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_state_choropleth(df, 'dur_hours', aggfunc = 'mean', title='Average Duration of Major Power Outages (Hours) by U.S. State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/p9_syfb94d74zplzygjkhnvh0000gn/T/ipykernel_1558/46275117.py:9: FutureWarning:\n",
      "\n",
      "The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urban_bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Very Low</th>\n",
       "      <td>296</td>\n",
       "      <td>41.36</td>\n",
       "      <td>9.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>319</td>\n",
       "      <td>59.55</td>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>276</td>\n",
       "      <td>29.53</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>315</td>\n",
       "      <td>51.39</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very High</th>\n",
       "      <td>270</td>\n",
       "      <td>33.36</td>\n",
       "      <td>5.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count   mean  median\n",
       "urban_bin                      \n",
       "Very Low     296  41.36    9.43\n",
       "Low          319  59.55   35.00\n",
       "Medium       276  29.53    6.32\n",
       "High         315  51.39   12.00\n",
       "Very High    270  33.36    5.74"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code groups states into 5 urbanization categories using quantile bins \n",
    "#and compute stats for outage durations for each group\n",
    "df['urban_bin'] = pd.qcut(\n",
    "    df['poppct_urban (%)'],\n",
    "    5,\n",
    "    labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "pivot = (\n",
    "    df.groupby('urban_bin')['dur_hours']\n",
    "    .agg(['count', 'mean', 'median'])\n",
    "    .round(2)\n",
    ")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['urban_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"620px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_76.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"620px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_76.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8302 0.0\n"
     ]
    }
   ],
   "source": [
    "col = 'pct_water_tot (%)'\n",
    "fig = create_kde_plotly(\n",
    "    df = df, \n",
    "    group_col = 'anomaly.level (numeric)', \n",
    "    group1 = True, \n",
    "    group2 = False, \n",
    "    vals_col = col, \n",
    "    title=f'KDE: anomaly.level vs {col}'\n",
    "    )\n",
    "fig.write_html(f'assets/hyp_{col}.html')\n",
    "fig.show()\n",
    "\n",
    "col = 'com.sales (megawatt-hour)'\n",
    "fig = create_kde_plotly(\n",
    "    df = df, \n",
    "    group_col = 'anomaly.level (numeric)', \n",
    "    group1 = True, \n",
    "    group2 = False, \n",
    "    vals_col = col, \n",
    "    title=f'KDE: anomaly.level vs {col}'\n",
    "    )\n",
    "fig.write_html(f'assets/hyp_{col}.html')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "def perm_test(df, col1, col2):\n",
    "    missing = df[df[col1].isna()][col2].values\n",
    "    not_missing = df[df[col1].notna()][col2].values\n",
    "    obs = stats.ks_2samp(missing, not_missing).statistic\n",
    "    comb = np.concatenate([missing, not_missing])\n",
    "    perm_stats = []\n",
    "    for _ in range(10_000):\n",
    "        perm = np.random.permutation(comb)\n",
    "        perm_miss = perm[:len(missing)]\n",
    "        perm_not_miss = perm[len(missing):]\n",
    "        perm_stat = stats.ks_2samp(perm_miss, perm_not_miss).statistic\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    p_val = np.mean(perm_stats >= obs)\n",
    "    return p_val        \n",
    "\n",
    "pval_no = perm_test(df, 'anomaly.level (numeric)', 'pct_water_tot (%)')\n",
    "pval_yes = perm_test(df, 'anomaly.level (numeric)', 'com.sales (megawatt-hour)')\n",
    "\n",
    "print(pval_no, pval_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"620px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_77.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0563\n"
     ]
    }
   ],
   "source": [
    "#perm test to examine whether outage duration depends on climate.category (there is an obvious yes atm)\n",
    "df_valid = df.dropna(subset=['climate.category', 'dur_hours']).copy()\n",
    "df_valid['is_normal_climate'] = (df_valid['climate.category']=='normal')\n",
    "\n",
    "fig = create_kde_plotly(\n",
    "    df = df_valid, \n",
    "    group_col = 'is_normal_climate', \n",
    "    group1 = True, \n",
    "    group2 = False, \n",
    "    vals_col = 'dur_hours', \n",
    "    title=f'KDE: climate.category vs dur_hours'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "#since dist shape is quite similar, I will be using ks stat\n",
    "def ks_perm_test_gen(df, col1, group, col2):\n",
    "    g1 = df[df[col1] == group][col2].dropna().values\n",
    "    g2 = df[df[col1]!=group][col2].dropna().values\n",
    "    obs = stats.ks_2samp(g1, g2).statistic\n",
    "    comb = np.concatenate([g1, g2])\n",
    "    perm_stats = []\n",
    "    for _ in range(10_000):\n",
    "        perm =np.random.permutation(comb)\n",
    "        perm_g1 = perm[:len(g1)]\n",
    "        perm_g2 = perm[len(g1):]\n",
    "        perm_stat = stats.ks_2samp(perm_g1, perm_g2).statistic\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    p_val = np.mean(perm_stats >= obs)\n",
    "    return p_val\n",
    "\n",
    "pval = ks_perm_test_gen(df_valid, 'is_normal_climate', True, 'dur_hours')\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "#predicting dur.hours based on almost all the feature present in the df\n",
    "df = df.drop(columns = ['cause.category.detail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "def model_pipeline(data, model):\n",
    "    categorical_features = [\n",
    "    'u.s._state',\n",
    "    'nerc.region',\n",
    "    'climate.region',\n",
    "    'climate.category',\n",
    "    'cause.category',\n",
    "    ]\n",
    "\n",
    "    num_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_features = [col for col in num_cols if col!='dur_hours']\n",
    "    \n",
    "    cat_proc = Pipeline(steps = [\n",
    "        ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    num_proc = Pipeline(steps = [\n",
    "        ('imputer', SimpleImputer(strategy = 'median')),\n",
    "        ('scale', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_proc, numeric_features),\n",
    "            ('cat', cat_proc, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\",model)\n",
    "    ])\n",
    "    return pipe\n",
    "    \n",
    "def train_model(data, model, gd=False, prm_grid=None):\n",
    "    data = data.dropna(subset=['dur_hours'])\n",
    "    X = data.drop(columns = ['dur_hours'])\n",
    "    y = data['dur_hours']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "    )        \n",
    "    if gd:\n",
    "        grid = GridSearchCV(\n",
    "            estimator = model,\n",
    "            param_grid = prm_grid,\n",
    "            scoring = 'neg_mean_absolute_error',\n",
    "            cv = 3,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        grid.fit(X_train, y_train)\n",
    "        print(\"Best Params:\", grid.best_params_)\n",
    "        model = grid.best_estimator_\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    prd_test, prd_train = model.predict(X_test), model.predict(X_train)\n",
    "    mae_train = np.mean(np.abs(prd_train-y_train))\n",
    "    mae_test = np.mean(np.abs(prd_test-y_test))\n",
    "    return model, mae_train, mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('preprocess',\n",
       "                  ColumnTransformer(transformers=[('num',\n",
       "                                                   Pipeline(steps=[('imputer',\n",
       "                                                                    SimpleImputer(strategy='median')),\n",
       "                                                                   ('scale',\n",
       "                                                                    StandardScaler())]),\n",
       "                                                   ['anomaly.level (numeric)',\n",
       "                                                    'demand.loss.mw (megawatt)',\n",
       "                                                    'customers.affected',\n",
       "                                                    'total.price (cents / '\n",
       "                                                    'kilowatt-hour)',\n",
       "                                                    'res.percen (%)',\n",
       "                                                    'res.cust.pct (%)',\n",
       "                                                    'pc.realgsp.state (usd)',\n",
       "                                                    'pc.realgsp.usa (usd)',...\n",
       "                                                    'square mile)',\n",
       "                                                    'areapct_urban (%)',\n",
       "                                                    'areapct_uc (%)',\n",
       "                                                    'pct_water_tot (%)',\n",
       "                                                    'pct_water_inland (%)']),\n",
       "                                                  ('cat',\n",
       "                                                   Pipeline(steps=[('imputer',\n",
       "                                                                    SimpleImputer(fill_value='Unknown',\n",
       "                                                                                  strategy='constant')),\n",
       "                                                                   ('encoder',\n",
       "                                                                    OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                   ['u.s._state', 'nerc.region',\n",
       "                                                    'climate.region',\n",
       "                                                    'climate.category',\n",
       "                                                    'cause.category'])])),\n",
       "                 ('model', LinearRegression())]),\n",
       " np.float64(37.544501787729985),\n",
       " np.float64(45.17810089070457))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "base_model = model_pipeline(df_exp, LinearRegression())\n",
    "train_model(df_exp, base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_135.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://plotly.com/python/heatmaps/\n",
    "#plotting the corr matrix to simply the model\n",
    "\n",
    "corr = df_exp.corr(numeric_only = True)\n",
    "fig = px.imshow(\n",
    "    corr,\n",
    "    text_auto = True,\n",
    "    color_continuous_scale = 'RdBu_r',\n",
    "    title = 'Corr Matrix for Numerical features'\n",
    ")\n",
    "\n",
    "fig.update_layout(width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df.copy(deep=True)\n",
    "df_exp = df_exp.drop(columns=[\n",
    "    'res.price (cents / kilowatt-hour)',\n",
    "    'com.price (cents / kilowatt-hour)',\n",
    "    'ind.price (cents / kilowatt-hour)',\n",
    "    'res.sales (megawatt-hour)',\n",
    "    'com.sales (megawatt-hour)', \n",
    "    'ind.sales (megawatt-hour)',\n",
    "    'total.sales (megawatt-hour)',\n",
    "    'res.customers', 'com.customers',\n",
    "    'ind.customers',\n",
    "    'com.cust.pct (%)', \n",
    "    'ind.cust.pct (%)',\n",
    "    'res.sales (megawatt-hour)',\n",
    "    'com.sales (megawatt-hour)', \n",
    "    'ind.sales (megawatt-hour)',\n",
    "    'com.percen (%)',\n",
    "    'ind.percen (%)',\n",
    "    'total.customers',\n",
    "    'util.realgsp (usd)',\n",
    "    'pi.util.ofusa (%)',\n",
    "    'pc.realgsp.rel (fraction)',\n",
    "    'pct_land (%)'\n",
    "])\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"model__criterion\": [\"absolute_error\"],\n",
    "    \"model__n_estimators\": [200, 400, 600],\n",
    "    \"model__max_depth\": [20, 40, None],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "# xgb = XGBRegressor(\n",
    "#     n_estimators=300,\n",
    "#     learning_rate=0.04,\n",
    "#     max_depth=6,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     reg_lambda=1.0,\n",
    "#     objective='reg:squarederror',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "rf = model_pipeline(df_exp, RandomForestRegressor())\n",
    "final_model, train_err, test_err = train_model(df_exp, xgb, gd = True, prm_grid = param_grid)\n",
    "print(train_err, test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Params: {'model__max_depth': 1000, 'model__n_estimators': 400}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
